{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JamMa\\AppData\\Local\\Temp\\ipykernel_33248\\3880542425.py:40: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self.driver = webdriver.Chrome(path_to_driver, chrome_options=options)\n",
      "C:\\Users\\JamMa\\AppData\\Local\\Temp\\ipykernel_33248\\3880542425.py:40: DeprecationWarning: use options instead of chrome_options\n",
      "  self.driver = webdriver.Chrome(path_to_driver, chrome_options=options)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No text to speak",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 110\u001b[0m\n\u001b[0;32m    108\u001b[0m viddit \u001b[39m=\u001b[39m Viddit()\n\u001b[0;32m    109\u001b[0m viddit\u001b[39m.\u001b[39maccept_cookies()\n\u001b[1;32m--> 110\u001b[0m viddit\u001b[39m.\u001b[39;49mscrape(link)\n",
      "Cell \u001b[1;32mIn[3], line 102\u001b[0m, in \u001b[0;36mViddit.scrape\u001b[1;34m(self, post_url)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m# Getting comment into string TODO TTS\u001b[39;00m\n\u001b[0;32m    101\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([element\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m comments[i]\u001b[39m.\u001b[39mfind_elements(By\u001b[39m.\u001b[39mCSS_SELECTOR,\u001b[39m'\u001b[39m\u001b[39m.RichTextJSON-root\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[1;32m--> 102\u001b[0m tts \u001b[39m=\u001b[39m gTTS(text)\n\u001b[0;32m    103\u001b[0m tts\u001b[39m.\u001b[39msave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(COMMENT_AUDIO_DIR, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.mp3\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    104\u001b[0m \u001b[39m# Screenshot & save text\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JamMa\\Anaconda3\\envs\\viddit\\lib\\site-packages\\gtts\\tts.py:128\u001b[0m, in \u001b[0;36mgTTS.__init__\u001b[1;34m(self, text, tld, lang, slow, lang_check, pre_processor_funcs, tokenizer_func)\u001b[0m\n\u001b[0;32m    125\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, k, v)\n\u001b[0;32m    127\u001b[0m \u001b[39m# Text\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m \u001b[39massert\u001b[39;00m text, \u001b[39m\"\u001b[39m\u001b[39mNo text to speak\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext \u001b[39m=\u001b[39m text\n\u001b[0;32m    131\u001b[0m \u001b[39m# Translate URL top-level domain\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: No text to speak"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import logging \n",
    "import os\n",
    "import time\n",
    "from gtts import gTTS\n",
    "\n",
    "#TODO: Remove Automod comments\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BASE_OUTPUT_DIR = \"output\"\n",
    "COMMENT_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"comments\")\n",
    "POST_DIR = os.path.join(BASE_OUTPUT_DIR, \"post\")\n",
    "POST_AUDIO_DIR = os.path.join(POST_DIR, \"audio\")\n",
    "POST_IMAGE_DIR = os.path.join(POST_DIR, \"images\")\n",
    "COMMENT_AUDIO_DIR = os.path.join(COMMENT_OUTPUT_DIR, \"audio\")\n",
    "COMMENT_IMAGE_DIR = os.path.join(COMMENT_OUTPUT_DIR, \"images\")\n",
    "DIRECTORIES = [POST_AUDIO_DIR, POST_IMAGE_DIR, COMMENT_AUDIO_DIR, COMMENT_IMAGE_DIR]\n",
    "\n",
    "link = \"https://www.reddit.com/r/MachineLearning/comments/11rtzv6/d_what_do_people_think_about_openai_not_releasing/\"\n",
    "\n",
    "def make_dir_if_not_exists(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "def delete_content_of_dir(dir_name):\n",
    "    if os.path.exists(dir_name):\n",
    "        for file in os.listdir(dir_name):\n",
    "            os.remove(os.path.join(dir_name, file))\n",
    "\n",
    "\n",
    "class Viddit:\n",
    "    def __init__(self, path_to_driver = \"chromedriver.exe\"):\n",
    "        options = Options()\n",
    "        # options.add_argument('--headless')\n",
    "        # options.add_argument('--disable-gpu')  # Last I checked this was necessary.\n",
    "        self.driver = webdriver.Chrome(path_to_driver, chrome_options=options)\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        [make_dir_if_not_exists(directory) for directory in DIRECTORIES]\n",
    "\n",
    "    def delete_data(self):\n",
    "        [delete_content_of_dir(directory) for directory in DIRECTORIES]\n",
    "    \n",
    "    def teardown(self):\n",
    "        self.delete_data()\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "\n",
    "    def accept_cookies(self):\n",
    "        self.driver.get(\"https://www.reddit.com/\")\n",
    "        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Accept all')]\")))\n",
    "        self.driver.find_element(\"xpath\", \"//button[contains(text(), 'Accept all')]\").click()\n",
    "        logger.info(\"Cookies accepted\")\n",
    "        self.driver.switch_to.default_content() \n",
    "\n",
    "    def scrape(self, post_url):\n",
    "        # Load cookies to prevent cookie overlay & other issues\n",
    "        # for cookie in config['reddit_cookies'].split('; '):\n",
    "        #     cookie_data = cookie.split('=')\n",
    "        #     driver.add_cookie({'name':cookie_data[0],'value':cookie_data[1],'domain':'reddit.com'})\n",
    "\n",
    "        # Fetching the post itself, text & screenshot\n",
    "        self.driver.get(post_url)\n",
    "        post = WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, '.Post')))\n",
    "        post_text = post.find_element(By.CSS_SELECTOR, 'h1').text #TODO Text to speech\n",
    "        post.screenshot(os.path.join(POST_IMAGE_DIR, \"0.png\"))\n",
    "        tts = gTTS(post_text)\n",
    "        tts.save(os.path.join(POST_AUDIO_DIR, \"0.mp3\"))\n",
    "        # Let comments load\n",
    "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3) #TODO Can be a WebDriverWait\n",
    "        \n",
    "        # Fetching comments & top level comment determinator\n",
    "        comments = WebDriverWait(self.driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[id^=t1_][tabindex]')))\n",
    "        allowed_style = comments[0].get_attribute(\"style\")\n",
    "        \n",
    "        # Filter for top only comments\n",
    "        NUMBER_OF_COMMENTS = 10\n",
    "        comments = [comment for comment in comments if comment.get_attribute(\"style\") == allowed_style][:NUMBER_OF_COMMENTS]\n",
    "\n",
    "        logger.info('ðŸ’¬ Scraping comments...')\n",
    "        # Save time & resources by only fetching X content\n",
    "        for i in range(len(comments)):\n",
    "            # TODO Filter out locked comments (AutoMod) \n",
    "            # Scrolling to the comment ensures that the profile picture loads\n",
    "            # Credits: https://stackoverflow.com/a/57630350\n",
    "            desired_y = (comments[i].size['height'] / 2) + comments[i].location['y']\n",
    "            window_h = self.driver.execute_script('return window.innerHeight')\n",
    "            window_y = self.driver.execute_script('return window.pageYOffset')\n",
    "            current_y = (window_h / 2) + window_y\n",
    "            scroll_y_by = desired_y - current_y\n",
    "            self.driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_y_by)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "            # Getting comment into string TODO TTS\n",
    "            text = \"\\n\".join([element.text for element in comments[i].find_elements(By.CSS_SELECTOR,'.RichTextJSON-root')])\n",
    "            tts = gTTS(text)\n",
    "            tts.save(os.path.join(COMMENT_AUDIO_DIR, f'{i}.mp3'))\n",
    "            # Screenshot & save text\n",
    "            comments[i].screenshot(os.path.join(COMMENT_IMAGE_DIR, f'{i}.png'))\n",
    "        return self.driver\n",
    "    \n",
    "viddit = Viddit()\n",
    "viddit.accept_cookies()\n",
    "viddit.scrape(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "viddit.delete_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "from tensorflow_tts.inference import TFAutoModel\n",
    "from tensorflow_tts.inference import AutoProcessor\n",
    "\n",
    "# initialize fastspeech2 model.\n",
    "fastspeech2 = TFAutoModel.from_pretrained(\"tensorspeech/tts-fastspeech2-ljspeech-en\")\n",
    "# initialize mb_melgan model\n",
    "mb_melgan = TFAutoModel.from_pretrained(\"tensorspeech/tts-mb_melgan-ljspeech-en\")\n",
    "# inference\n",
    "processor = AutoProcessor.from_pretrained(\"tensorspeech/tts-fastspeech2-ljspeech-en\")\n",
    "input_ids = processor.text_to_sequence(\"Hello from a computer.\")\n",
    "# fastspeech inference\n",
    "mel_before, mel_after, duration_outputs, _, _ = fastspeech2.inference(\n",
    "    input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),\n",
    "    speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),\n",
    "    speed_ratios=tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    "    f0_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    "    energy_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    ")\n",
    "\n",
    "# melgan inference\n",
    "audio_before = mb_melgan.inference(mel_before)[0, :, 0]\n",
    "audio_after = mb_melgan.inference(mel_after)[0, :, 0]\n",
    "\n",
    "# save to file\n",
    "sf.write('./audio_before.wav', audio_before, 22050, \"PCM_16\")\n",
    "sf.write('./audio_after.wav', audio_after, 22050, \"PCM_16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip, concatenate_videoclips\n",
    "\n",
    "def overlay_images_on_video(background_video_path, png_paths, audio_paths, audio_volume, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(background_video_path)\n",
    "\n",
    "    # Get the video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    logger.debug(f'Video properties: {fps} fps, {width}x{height} pixels')\n",
    "\n",
    "\n",
    "    # Create a list of video clips for each PNG\n",
    "    clips = []\n",
    "    for i in range(len(png_paths)):\n",
    "        # Read the PNG image\n",
    "        png = cv2.imread(png_paths[i], cv2.IMREAD_UNCHANGED)\n",
    "    # Calculate the total number of frames for the display time of each PNG\n",
    "\n",
    "    # Open the audio file and set its volume\n",
    "        audio_clip = AudioFileClip(audio_paths[i]).volumex(audio_volume)\n",
    "        num_frames = int(audio_clip.duration * fps) + fps\n",
    "        # Calculate the size and position of the PNG image to be centered on the video\n",
    "        png_height, png_width, _ = png.shape\n",
    "        x = int((width - png_width) / 2)\n",
    "        y = int((height - png_height) / 2)\n",
    "\n",
    "        # Create a mask for the PNG image\n",
    "        alpha_channel = png[:, :, 3]\n",
    "        mask = np.zeros((png_height, png_width), dtype=np.uint8)\n",
    "        mask[alpha_channel > 0] = 255\n",
    "\n",
    "        # Create a list of video frames for the current PNG\n",
    "        frames = []\n",
    "        for i in range(num_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # Resize the PNG image to match the size of the video frame\n",
    "                png_resized = cv2.resize(png[:, :, :3], (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "                # Apply the mask to the PNG image\n",
    "                mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]))\n",
    "                mask_resized = cv2.cvtColor(mask_resized, cv2.COLOR_GRAY2BGR)\n",
    "                png_masked = cv2.bitwise_and(png_resized, mask_resized)\n",
    "\n",
    "                # Overlay the PNG image on the video frame\n",
    "                frame[y:y+png_height, x:x+png_width] = cv2.add(frame[y:y+png_height, x:x+png_width], png_masked)\n",
    "\n",
    "                # Add the frame to the list of frames\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Create a video clip from the list of frames and add it to the list of clips\n",
    "        clip = VideoFileClip(None, fps=fps).set_duration(num_frames/fps)\n",
    "        clip = clip.set_position('center')\n",
    "        clip = clip.set_make_frame(lambda t: frames[int(t * fps)])\n",
    "        clip.set_audio(audio_clip)\n",
    "        clips.append(clip)\n",
    "\n",
    "    # Concatenate the list of video clips and set the audio\n",
    "    video_clip = concatenate_videoclips(clips)\n",
    "    video_clip = video_clip.set_audio(CompositeAudioClip([audio_clip for _ in range(len(clips))]))\n",
    "    video_clip.write_videofile(output_path, fps=fps)\n",
    "    video_clip.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viddit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
